{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as rq\n",
    "import os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O projeto usará como base os seguintes sites:\n",
    "MLSSoccer - https://www.mlssoccer.com\n",
    "Eurosport - https://www.eurosport.com\n",
    "Soccerway - https://us.soccerway.com\n",
    "FCTable - https://www.fctables.com\n",
    "WhoScored - https://www.whoscored.com\n",
    "FIFA - https://www.fifa.com\n",
    "TopDrawerSoccer - https://www.topdrawersoccer.com\n",
    "FoxSports - https://www.foxsports.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [('MLSSoccer','https://www.mlssoccer.com'),\n",
    "('Eurosport','https://www.eurosport.com'),\n",
    "('Soccerway','https://us.soccerway.com'),\n",
    "('FCTable','https://www.fctables.com'),\n",
    "('WhoScored','https://www.whoscored.com'),\n",
    "('FIFA','https://www.fifa.com'),\n",
    "('TopDrawerSoccer','https://www.topdrawersoccer.com'),\n",
    "('FoxSports','https://www.foxsports.com')]\n",
    "##Duvidas:\n",
    "#Eurosport vai ter como root https://www.eurosport.com/ ou https://www.eurosport.com/football ?\n",
    "#Foxsports vai ter como root https://www.foxsports.com/ ou https://www.foxsports.com/soccer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Robots.txt de cada site\n",
    "#for i in range(len(sites)):\n",
    "# urlRobots = sites[i][1] + '/robots.txt'\n",
    "# headers = {'User-Agent': 'bvcl'}\n",
    "# req = rq.get(urlRobots, headers=headers)\n",
    "# directoryFile = 'robots/' + sites[i][0] + \".txt\"\n",
    "# with open(directoryFile, 'wb') as handle:\n",
    "#    for block in req.iter_content():\n",
    "#        handle.write(block)\n",
    "\n",
    "###Problema no site whoscored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pagina inicial de cada site\n",
    "#for i in range(len(sites)):\n",
    "#    urlHomePage = sites[i][1]\n",
    "#    req = rq.get(urlHomePage)\n",
    "#    directoryFile = 'pages/' + sites[i][0] + \"/homePage.html\" \n",
    "#    os.makedirs(os.path.dirname(directoryFile), exist_ok=True)\n",
    "#    with open(directoryFile, 'wb') as handle:\n",
    "#        for block in req.iter_content():\n",
    "#            handle.write(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Problema no whoscored\n",
    "urlHomePage = sites[0][1]\n",
    "req = rq.get(urlHomePage)\n",
    "parsedPage = BeautifulSoup(req.text,\"html.parser\")\n",
    "anchors = parsedPage.find_all('a', href = True)\n",
    "#anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construindo array de allow e disallow do robots.txt\n",
    "###Duvida: Levar em consideração Visit-time e Crawl-delay do robots ?\n",
    "pageCounter = 0\n",
    "robotsTXT = 'robots/' + sites[pageCounter][0] + \".txt\"\n",
    "with open(robotsTXT) as f:\n",
    "   content = f.readlines()\n",
    "\n",
    "start = len(content)\n",
    "end = len(content)\n",
    "disallowArray = []\n",
    "allowArray = []\n",
    "visitTime = ''\n",
    "crawlDelay = ''\n",
    "for i in range(len(content)):\n",
    "   if('User-agent: *' in content[i]):\n",
    "       start = i\n",
    "       break\n",
    "for i in range(start+1, len(content)):\n",
    "   if('User-agent: ' in content[i]):\n",
    "       end = i\n",
    "       break\n",
    "for i in range(start+1, len(content)):\n",
    "   if('Visit-time: ' in content[i]):\n",
    "       visitTime = content[i].split(' ')[1]\n",
    "       break\n",
    "for i in range(start+1, len(content)):\n",
    "   if('Crawl-delay: ' in content[i]):\n",
    "       crawlDelay = content[i].split(' ')[1].split('\\n')[0]\n",
    "       break\n",
    "for i in range (start+1, end-1):\n",
    "   line = content[i].split(' ')\n",
    "   if(line[0] == 'Allow:'):\n",
    "       allowArray.append(line[1].split('\\n')[0])\n",
    "   elif(line[0] == 'Disallow:'):\n",
    "       disallowArray.append(line[1].split('\\n')[0])\n",
    "#disallowArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
