{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as rq\n",
    "import os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O projeto usará como base os seguintes sites:\n",
    "MLSSoccer - https://www.mlssoccer.com\n",
    "Eurosport - https://www.eurosport.com\n",
    "Soccerway - https://us.soccerway.com\n",
    "FCTable - https://www.fctables.com\n",
    "WhoScored - https://www.whoscored.com\n",
    "FIFA - https://www.fifa.com\n",
    "TopDrawerSoccer - https://www.topdrawersoccer.com\n",
    "FoxSports - https://www.foxsports.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sites = [('MLSSoccer','https://www.mlssoccer.com'),\n",
    "('Eurosport','https://www.eurosport.com'),\n",
    "('Soccerway','https://us.soccerway.com'),\n",
    "('FCTable','https://www.fctables.com'),\n",
    "('WhoScored','https://www.whoscored.com'),\n",
    "('FIFA','https://www.fifa.com'),\n",
    "('TopDrawerSoccer','https://www.topdrawersoccer.com'),\n",
    "('FoxSports','https://www.foxsports.com')]\n",
    "##Duvidas:\n",
    "#Eurosport vai ter como root https://www.eurosport.com/ ou https://www.eurosport.com/football ?\n",
    "#Foxsports vai ter como root https://www.foxsports.com/ ou https://www.foxsports.com/soccer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Robots.txt de cada site\n",
    "#for i in range(len(sites)):\n",
    "# urlRobots = sites[i][1] + '/robots.txt'\n",
    "# headers = {'User-Agent': 'bvcl'}\n",
    "# req = rq.get(urlRobots, headers=headers)\n",
    "# directoryFile = 'robots/' + sites[i][0] + \".txt\"\n",
    "# with open(directoryFile, 'wb') as handle:\n",
    "#    for block in req.iter_content():\n",
    "#        handle.write(block)\n",
    "\n",
    "###Problema no site whoscored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pagina inicial de cada site\n",
    "#for i in range(len(sites)):\n",
    "#    urlHomePage = sites[i][1]\n",
    "#    req = rq.get(urlHomePage)\n",
    "#    directoryFile = 'pages/' + sites[i][0] + \"/homePage.html\" \n",
    "#    os.makedirs(os.path.dirname(directoryFile), exist_ok=True)\n",
    "#    with open(directoryFile, 'wb') as handle:\n",
    "#        for block in req.iter_content():\n",
    "#            handle.write(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Problema no whoscored\n",
    "urlHomePage = sites[0][1]\n",
    "req = rq.get(urlHomePage)\n",
    "parsedPage = BeautifulSoup(req.text,\"html.parser\")\n",
    "anchors = parsedPage.find_all('a', href = True)\n",
    "#anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construindo array de allow e disallow do robots.txt\n",
    "###Duvida: Levar em consideração Visit-time e Crawl-delay do robots ?\n",
    "def criarDisallowArray(pageCounter):\n",
    "    robotsTXT = 'robots/' + sites[pageCounter][0] + \".txt\"\n",
    "    with open(robotsTXT) as f:\n",
    "       content = f.readlines()\n",
    "\n",
    "    start = len(content)\n",
    "    end = len(content)\n",
    "    disallowArray = []\n",
    "    allowArray = []\n",
    "    visitTime = ''\n",
    "    crawlDelay = ''\n",
    "    for i in range(len(content)):\n",
    "       if('User-agent: *' in content[i]):\n",
    "           start = i\n",
    "           break\n",
    "    for i in range(start+1, len(content)):\n",
    "       if('User-agent: ' in content[i]):\n",
    "           end = i\n",
    "           break\n",
    "    for i in range(start+1, len(content)):\n",
    "       if('Visit-time: ' in content[i]):\n",
    "           visitTime = content[i].split(' ')[1]\n",
    "           break\n",
    "    for i in range(start+1, len(content)):\n",
    "       if('Crawl-delay: ' in content[i]):\n",
    "           crawlDelay = content[i].split(' ')[1].split('\\n')[0]\n",
    "           break\n",
    "    for i in range (start+1, end-1):\n",
    "       line = content[i].split(' ')\n",
    "       if(line[0] == 'Allow:'):\n",
    "           allowArray.append(line[1].split('\\n')[0])\n",
    "       elif(line[0] == 'Disallow:'):\n",
    "           disallowArray.append(line[1].split('\\n')[0])\n",
    "    return disallowArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.mlssoccer.com/\n",
      "https://www.mlssoccer.com/\n",
      "#\n",
      "https://www.coloradorapids.com/\n",
      "https://www.fcdallas.com/\n",
      "https://www.houstondynamo.com/\n",
      "https://www.lafc.com/\n",
      "https://www.lagalaxy.com/\n",
      "https://www.mnufc.com/\n",
      "https://www.timbers.com/\n",
      "https://www.rsl.com/\n",
      "https://www.sjearthquakes.com/\n",
      "https://www.soundersfc.com/\n",
      "https://www.sportingkc.com/\n",
      "https://www.whitecapsfc.com/\n",
      "#\n",
      "https://atlutd.com/\n",
      "https://www.chicago-fire.com/\n",
      "https://www.fccincinnati.com/\n",
      "https://www.columbuscrewsc.com/\n"
     ]
    }
   ],
   "source": [
    "# for j in range(len(anchors)):\n",
    "#     for i in range (len(disallowArray)):\n",
    "#         if(disallowArray[i] in anchors[j]['href']):\n",
    "#             print(anchors[j]['href'])\n",
    "# for j in range(20):\n",
    "#     print(anchors[j]['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Abordagem BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funções auxiliares \n",
    "def isAllowed(disallowArr,url):\n",
    "    for i in range (len(disallowArr)):\n",
    "        if(disallowArr[i] in url):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isRootSite(siteUrl):\n",
    "    return not('http://' in siteUrl or 'https://' in siteUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinks(page):\n",
    "    parsedPage = BeautifulSoup(page,\"html.parser\")\n",
    "    anchors = parsedPage.find_all('a', href = True)\n",
    "    links = []\n",
    "    for anchor in anchors:\n",
    "        links.append(anchor['href'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPage(urlPage):\n",
    "    req = rq.get(urlPage)\n",
    "    return req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadPage(urlPage,siteFolder,pageCounter):\n",
    "    req = rq.get(urlPage)\n",
    "    directoryFile = 'pages/' + siteFolder + \"/page\" + str(pageCounter) + \".html\" \n",
    "    os.makedirs(os.path.dirname(directoryFile), exist_ok=True)\n",
    "    with open(directoryFile, 'wb') as handle:\n",
    "       for block in req.iter_content():\n",
    "           handle.write(block)\n",
    "    return req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# urlHomePage = sites[0][1]\n",
    "# isAllowed(disallowArray,'/account/me')\n",
    "# isRootSite('/league/history')\n",
    "# p = getPage(urlHomePage)\n",
    "# getLinks(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "#falta i=4 e i=5\n",
    "for i in range (7,len(sites)):\n",
    "    if(i!=4):   \n",
    "        urlQueue = queue.Queue()\n",
    "        visited = set()\n",
    "        downloaded = []\n",
    "        disallowArrayToUse = criarDisallowArray(i)\n",
    "        siteFolder = sites[i][0]\n",
    "        siteRootUrl = sites[i][1]\n",
    "        visited.add(siteRootUrl)\n",
    "        urlQueue.put(siteRootUrl)\n",
    "        pageCounter = 0;\n",
    "        while((not urlQueue.empty()) and pageCounter<999): \n",
    "            currentUrl = urlQueue.get()\n",
    "            page = downloadPage(currentUrl,siteFolder,pageCounter)\n",
    "            downloaded.append(currentUrl)\n",
    "            pageCounter+=1\n",
    "            print(pageCounter)\n",
    "            for url in getLinks(page):\n",
    "                if ((siteRootUrl+url not in visited) and isRootSite(url) and isAllowed(disallowArrayToUse,url)):\n",
    "                    visited.add(siteRootUrl+url)\n",
    "                    urlQueue.put(siteRootUrl+url)\n",
    "\n",
    "#downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
